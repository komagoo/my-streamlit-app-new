import os
import sys
import streamlit as st
from dotenv import load_dotenv
from pinecone import Pinecone
from pinecone import ServerlessSpec

# LangChainì˜ VectorStoreìš© Pinecone (ì´ë¦„ ì¶©ëŒ ìœ„í—˜ ìˆìœ¼ë‹ˆ ë³„ëª… ì‚¬ìš©)
from langchain_community.vectorstores import Pinecone as LangchainPinecone

# ----------------------------

# ë²„ì „ í™•ì¸
st.write("Python version:", sys.version)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env)
load_dotenv()

# ì¼ë°˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd
import re
from collections import defaultdict, Counter
import plotly.express as px
import base64

# Langchain ê´€ë ¨
from langchain.docstore.document import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# Pinecone ì´ˆê¸°í™”
pinecone_api_key = os.getenv("PINECONE_API_KEY")
if not pinecone_api_key:
    st.error("âŒ PINECONE_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. Streamlit Secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# Pinecone client ê°ì²´ ìƒì„±
pc = Pinecone(api_key=pinecone_api_key)

index_name = "maintenance-index"

try:
    if index_name not in pc.list_indexes():
        pc.create_index(
            name=index_name,
            dimension=1536,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1")
        )
except Exception as e:
    if "ALREADY_EXISTS" not in str(e):
        raise



# ì¸ë±ìŠ¤ ê°ì²´ ê°€ì ¸ì˜¤ê¸°
index = pc.Index(index_name)

# ë¡œê³  ì´ë¯¸ì§€ base64 ì¸ì½”ë”©
def get_base64_of_bin_file(bin_file_path):
    with open(bin_file_path, 'rb') as f:
        data = f.read()
    return base64.b64encode(data).decode()

logo_path = "Hero_logo(final).png"  # ì‹¤í–‰ í´ë” ê¸°ì¤€ ìƒëŒ€ê²½ë¡œ
logo_base64 = get_base64_of_bin_file(logo_path)



# ----------------------------
# 0. Streamlit ì„¤ì •
# ----------------------------
st.set_page_config(page_title="ğŸš€mySUNI X SK í”„ë¡œì íŠ¸", layout="wide")

# ìƒë‹¨ ë¹¨ê°„ìƒ‰ ë¼ì¸ + ë¡œê³ /ì œëª© ì˜ì—­
st.markdown(
    f"""
    <div style="background-color:#ff4b4b; height:30px; width:100%;"></div>
    <div style="display:flex; align-items:center; padding:20px 30px;">
        <img src="data:image/png;base64,{logo_base64}" alt="logo" style="height:100px; margin-right:30px;">
        <div>
            <h1 style="margin:0; font-size:2.5rem; color:#222;">HERO</h1>
            <p style="margin:0; font-size:1.1rem; color:#555;">Hynix Equipment Response Operator</p>
        </div>
    </div>
    """,
    unsafe_allow_html=True
)

# ----------------------------
# 0.1 ë¡œê·¸ì¸ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ----------------------------
if "logged_in" not in st.session_state:
    st.session_state.logged_in = False
if "api_key" not in st.session_state:
    st.session_state.api_key = None

# ----------------------------
# 1. ë¡œê·¸ì¸ ë‹¨ê³„
# ----------------------------
if not st.session_state.logged_in:
    st.subheader("ğŸ”‘ ë¡œê·¸ì¸")

    username = st.text_input("ì•„ì´ë””")
    password = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password")

    valid_users = {
        "mySUNI250728!@": "mySUNI250728!@",
    }

    if st.button("ë¡œê·¸ì¸"):
        if username in valid_users and password == valid_users[username]:
            st.session_state.logged_in = True
            st.session_state.username = username
            # âœ… secretsì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
            st.session_state.api_key = st.secrets["OPENAI_API_KEY"]
            st.success(f"âœ… {username}ë‹˜, í™˜ì˜í•©ë‹ˆë‹¤!")
            st.rerun()
        else:
            st.error("âŒ ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

# ----------------------------
# OpenAI API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸° (ì„¸ì…˜, í™˜ê²½ë³€ìˆ˜, Streamlit Secrets ìˆœì„œë¡œ í™•ì¸)
api_key = (
    st.session_state.get("api_key")
    or os.getenv("OPENAI_API_KEY")
    or (st.secrets["OPENAI_API_KEY"] if "OPENAI_API_KEY" in st.secrets else None)
)

if api_key:
    os.environ["OPENAI_API_KEY"] = api_key
else:
    st.error(
        "âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì•„ë˜ ë°©ë²• ì¤‘ í•˜ë‚˜ë¥¼ í™•ì¸í•˜ì„¸ìš”:\n"
        "- ë¡œê·¸ì¸ í›„ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n"
        "- ë¡œì»¬ ê°œë°œ ì‹œ `.env` íŒŒì¼ì— API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.\n"
        "- Streamlit Cloud Secretsì— API í‚¤ë¥¼ ì¶”ê°€í•˜ì„¸ìš”."
    )
    st.stop()




# ë©”ì¸ íƒ€ì´í‹€ (ë¡œê·¸ì¸ í›„ ìµœìƒë‹¨)
# ----------------------------
st.title("ğŸ›  HERO (Hynix Equipment Response Operator)")
st.caption("í•˜ì´ë‹‰ìŠ¤ ì¥ë¹„ ë¬¸ì œ, HEROì™€ í•¨ê»˜ í•´ê²°í•´ìš”!")


# ----------------------------
# 2. ì—‘ì…€ ì—…ë¡œë“œ
# ----------------------------
uploaded_file = st.file_uploader("ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ (.xlsx)", type=["xlsx"])
if uploaded_file is None:
    st.info("ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.")
    st.stop()

with st.spinner("ğŸ“‚ íŒŒì¼ ì½ê¸°/ì „ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤. ë°ì´í„°ê°€ ë§ì„ìˆ˜ë¡ ì˜¤ë˜ ê±¸ë ¤ìš”..."):
    df = pd.read_excel(uploaded_file)
    if 'ì •ë¹„ì¼ì' in df.columns:
        df['ì •ë¹„ì¼ì'] = pd.to_datetime(df['ì •ë¹„ì¼ì'], errors='coerce')

    df = df.dropna(subset=['ì •ë¹„ë…¸íŠ¸'])
    st.success(f"ì—…ë¡œë“œ ì™„ë£Œ: ì´ {len(df)} í–‰")

# ----------------------------
# 3. ë¬¸ì œ ì›ì¸ ì»¬ëŸ¼ ìƒì„± (ì—‘ì…€ ì—…ë¡œë“œ ì§í›„)
# ----------------------------
problem_keywords = [
    "wafer not", "plasma ignition failure", "pumpdown ì‹œê°„ ì§€ì—°",
    "mass flow controller ì´ìƒ", "etch residue over spec",
    "temperature drift", "slot valve ë™ì‘ ë¶ˆëŸ‰",
    "chamber pressure fluctuation", "he flow deviation", "RF auto match ë¶ˆëŸ‰"
]
alias_map = {
    "wafer not ê°ì§€ë¨": "wafer not",
    "wafer not ë°œìƒ": "wafer not",
    "rf auto match fail": "RF auto match ë¶ˆëŸ‰",
    "slot valve ë¶ˆëŸ‰": "slot valve ë™ì‘ ë¶ˆëŸ‰",
    "he flow dev": "he flow deviation",
}

def normalize_note(note: str) -> str:
    note = str(note).lower()
    note = re.sub(r'\s+', ' ', note)
    return note

def extract_cause(note: str):
    note_low = normalize_note(note)
    for alias, norm in alias_map.items():
        if alias in note_low:
            return norm
    for keyword in problem_keywords:
        if keyword.lower() in note_low:
            return keyword
    return "ê¸°íƒ€"

df['ë¬¸ì œì›ì¸'] = df['ì •ë¹„ë…¸íŠ¸'].apply(extract_cause)

# ----------------------------
# 4. ì •ë¹„ë…¸íŠ¸ ê¸°ë°˜ ì„±ê³µë¥  ê³„ì‚°
# ----------------------------
all_texts = [str(note).strip() for note in df['ì •ë¹„ë…¸íŠ¸']]

cause_pattern = re.compile(r'LOT ì§„í–‰ ì¤‘ (.+) ë°œìƒ')
first_action_pattern = re.compile(r'1ì°¨ ì¡°ì¹˜: (.+) â†’ ì—¬ì „íˆ ì´ìƒ ë°œìƒ')
second_action_pattern = re.compile(r'ì •ë¹„ ì‹œì‘\. (.+) ì§„í–‰')
third_action_pattern = re.compile(r'ì¶”ê°€ ì¡°ì¹˜: (.+)')

cause_aliases = {
    "wafer not ë°œìƒ": "wafer not",
    "wafer not ê°ì§€ë¨": "wafer not",
    "wafer not ë°œìƒ í™•ì¸": "wafer not",
}

def normalize_cause(cause):
    for alias, norm in cause_aliases.items():
        if alias in cause:
            return norm
    return cause

cause_action_counts = defaultdict(lambda: defaultdict(Counter))
note_map = defaultdict(list)

for idx, note in enumerate(all_texts):
    lines = [line.strip() for line in note.split('\n') if line.strip()]
    cause = None
    for line in lines:
        cause_match = cause_pattern.search(line)
        if cause_match:
            cause = normalize_cause(cause_match.group(1).strip())
            continue
        if cause is None:
            continue

        action = None
        m1 = first_action_pattern.search(line)
        m2 = second_action_pattern.search(line)
        m3 = third_action_pattern.search(line)

        if m1:
            action = m1.group(1).strip()
            cause_action_counts[cause][action]['first'] += 1
        elif m2:
            action = m2.group(1).strip()
            cause_action_counts[cause][action]['second'] += 1
        elif m3:
            action = m3.group(1).strip()
            cause_action_counts[cause][action]['third'] += 1

        if action:
            note_map[(cause, action)].append(note)

rows = []
for cause, actions in cause_action_counts.items():
    for action, counts in actions.items():
        first_count = counts['first']
        second_count = counts['second']
        third_count = counts['third']
        total = first_count + second_count + third_count
        success = second_count + third_count
        success_rate = round(success / total * 100, 2) if total > 0 else 0

        rows.append({
            "ëŒ€í‘œì›ì¸": cause,
            "ì¡°ì¹˜": action,
            "ì´íšŸìˆ˜": total,
            "ì‹¤íŒ¨íšŸìˆ˜": first_count,
            "ì„±ê³µíšŸìˆ˜": success,
            "ì„±ê³µë¥ (%)": success_rate,
            "ì •ë¹„ë…¸íŠ¸": note_map[(cause, action)][0] if note_map[(cause, action)] else ""
        })

df_success = pd.DataFrame(rows)
# ----------------------------
# 5. LangChain RAG ì¤€ë¹„ (Pinecone ê¸°ë°˜, ì„¸ì…˜ ìºì‹± í¬í•¨)
# ----------------------------

# âœ… Pinecone API ì´ˆê¸°í™” (Streamlit Secretsì— ë“±ë¡ëœ í‚¤ë¥¼ ë¶ˆëŸ¬ì˜´)
pinecone_api_key = os.getenv("PINECONE_API_KEY")
#pinecone_environment = os.getenv("PINECONE_ENVIRONMENT")
pc = Pinecone(api_key=pinecone_api_key)

# âœ… ì‚¬ìš©í•  ì¸ë±ìŠ¤ ì´ë¦„ ì„¤ì • (ì—†ìœ¼ë©´ ìµœì´ˆ ì‹¤í–‰ ì‹œ ìƒì„±ë¨)
index_name = "maintenance-index"
existing_indexes = pc.list_indexes()  # .names() ì—†ì´ë„ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
if index_name not in existing_indexes:
    pc.create_index(
        name=index_name,
        dimension=1536,
        metric="cosine",  # ê¼­ í¬í•¨
        spec=ServerlessSpec(cloud="aws", region="us-east-1")  # ë¬´ë£Œ í”Œëœ í—ˆìš© region
    )

# âœ… Pinecone ì¸ë±ìŠ¤ ê°ì²´ ê°€ì ¸ì˜¤ê¸°
index = pc.Index(index_name)

# âœ… ì •ë¹„ë…¸íŠ¸ë¥¼ LangChain ë¬¸ì„œ ê°ì²´ë¡œ ë³€í™˜
documents = [
    Document(page_content=str(row['ì •ë¹„ë…¸íŠ¸']), metadata={'row': idx})
    for idx, row in df.iterrows()
]

# âœ… ë¬¸ì„œ ë¶„í•  (ë„ˆë¬´ ê¸¸ë©´ ìª¼ê°œê¸°)
splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
split_docs = splitter.split_documents(documents)

# âœ… ì„¸ì…˜ì— embeddingê³¼ vectordbê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
if "embedding_model" not in st.session_state or "vectordb" not in st.session_state:
    with st.spinner("ğŸ” Pinecone ì„ë² ë”© ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
        # âœ… OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        embedding_model = OpenAIEmbeddings(model="text-embedding-3-large")

        # âœ… Pineconeì— ë²¡í„° ì €ì¥ì†Œ ì—…ë¡œë“œ (ìë™ ì„ë² ë”© í¬í•¨)
        vectordb = Pinecone.from_documents(
            documents=split_docs,
            embedding=embedding_model,
            index_name=index_name  # ğŸ“‚ ì €ì¥ëœ ì¸ë±ìŠ¤ ì´ë¦„
        )

        # âœ… ì„¸ì…˜ì— ì €ì¥ (ì•± ì‹¤í–‰ ì¤‘ ì¬ì‚¬ìš©)
        st.session_state["embedding_model"] = embedding_model
        st.session_state["vectordb"] = vectordb
else:
    # âœ… ì´ì „ì— ìƒì„±ëœ ì„ë² ë”© ë° ë²¡í„° DB ì‚¬ìš©
    embedding_model = st.session_state["embedding_model"]
    vectordb = st.session_state["vectordb"]

# âœ… LLM + ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ QA ì²´ì¸ ìƒì„±
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectordb.as_retriever(search_kwargs={'k': 20}),  # ğŸ” ìœ ì‚¬ë„ ê¸°ë°˜ top 20
    return_source_documents=True
)

# ----------------------
# 6. ì‚¬ì´ë“œë°” ë©”ë‰´
# ----------------------
menu = st.sidebar.radio(
    "ğŸ“‚ ë©”ë‰´ ì„ íƒ",
    ["ğŸ”¹ ì •ë¹„ ê²€ìƒ‰ & ì¶”ì²œ", "ğŸ“ˆ ì •ë¹„ í†µê³„ ìë£Œ"]
)

# ----------------------
# 7. ì •ë¹„ ê²€ìƒ‰ & ì¶”ì²œ í˜ì´ì§€
# ----------------------
if menu == "ğŸ”¹ ì •ë¹„ ê²€ìƒ‰ & ì¶”ì²œ":
    st.subheader("ğŸ¤– HERO ì±—ë´‡ â€“ ì •ë¹„ ë¬¸ì œ, ì œê°€ ë‹¤ ì•Œê³ ìˆì–´ìš”!")

    example_keywords = [
        "wafer not", "plasma ignition failure",
        "pumpdown ì‹œê°„ ì§€ì—°", "slot valve ë™ì‘ ë¶ˆëŸ‰",
        "RF auto match ë¶ˆëŸ‰"
    ]
 # ------------------ í•­ìƒ ë³´ì´ëŠ” ì´ˆë°˜ ì¸ì‚¬ë§ ------------------
    example_keywords = [
        "wafer not", "plasma ignition failure",
        "pumpdown ì‹œê°„ ì§€ì—°", "slot valve ë™ì‘ ë¶ˆëŸ‰",
        "RF auto match ë¶ˆëŸ‰"
    ]
    st.markdown(f"""
    <div style="display:flex; justify-content:flex-start; margin-bottom:10px;">
        <div style="font-size:30px; margin-right:8px;">ğŸ¤–</div>
        <div style="background-color:#F1F0F0; color:black; padding:10px 15px;
                    border-radius:15px; max-width:80%;">
            ì•ˆë…•í•˜ì„¸ìš”ğŸ‘‹<br>
            ë°˜ë„ì²´ ì¥ë¹„ ì •ë¹„ ì´ìŠˆ í•´ê²° ë„ìš°ë¯¸ HEROì…ë‹ˆë‹¤!<br>
            ì •ë¹„ ì´ìŠˆë¥¼ ì…ë ¥í•˜ì‹œë©´, HEROê°€ ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ì°¾ì•„ í•´ê²°ì±…ì„ ì œì•ˆí•´ë“œë ¤ìš”.<br><br>
            ğŸ’¡ {' | '.join(example_keywords)}
        </div>
    </div>
    """, unsafe_allow_html=True)

    # ------------------ ë‹µë³€ ìƒì„± ë¡œë”© ìŠ¤í”¼ë„ˆ ------------------
    query = st.text_input(
        "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”", key="search_query",
        placeholder="ì˜ˆ: slot valve ë™ì‘ì´ ì•ˆë¼ã… ã… "
    )

    # âœ… ì…ë ¥ì´ ìˆì„ ë•Œë§Œ ì‹¤í–‰
    if query.strip():
        with st.spinner("ğŸ”„ ìœ ì‚¬ ì •ë¹„ ì‚¬ë¡€ì™€ ë‹µë³€ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”..."):
            # ì‚¬ìš©ì ë§í’ì„ 
            st.markdown(f"""
            <div style="display:flex; justify-content:flex-end; margin-bottom:10px;">
                <div style="background-color:#DCF8C6; color:black;
                            padding:10px 15px; border-radius:15px;
                            max-width:70%; text-align:right;">
                    {query}
                </div>
                <div style="font-size:30px; margin-left:8px;">ğŸ‘¤</div>
            </div>
            """, unsafe_allow_html=True)

            # ----------------------
            # 1) RAG ê²€ìƒ‰
            # ----------------------
            output = qa_chain({"query": query})
            docs = output['source_documents']

            recommended = []
            seen_pairs = set()

            for doc in docs:
                note = doc.page_content.strip()
                for _, row in df_success.iterrows():
                    if row["ì¡°ì¹˜"] in note:
                        key = (row["ì¡°ì¹˜"], note)
                        if key in seen_pairs:
                            continue
                        seen_pairs.add(key)

                        matched_row = df[df['ì •ë¹„ë…¸íŠ¸'].astype(str).str.strip() == note]
                        equip_id = matched_row['ì¥ë¹„ID'].iloc[0] if 'ì¥ë¹„ID' in df.columns and not matched_row.empty else 'N/A'
                        model = matched_row['ëª¨ë¸'].iloc[0] if 'ëª¨ë¸' in df.columns and not matched_row.empty else 'N/A'
                        maint_type = matched_row['ì •ë¹„ì¢…ë¥˜'].iloc[0] if 'ì •ë¹„ì¢…ë¥˜' in df.columns and not matched_row.empty else 'N/A'
                        maint_person = matched_row['ì •ë¹„ì'].iloc[0] if 'ì •ë¹„ì' in df.columns and not matched_row.empty else 'N/A'

                        recommended.append({
                            "ì¡°ì¹˜": row["ì¡°ì¹˜"],
                            "ì„±ê³µë¥ ": row["ì„±ê³µë¥ (%)"],
                            "ì •ë¹„ë…¸íŠ¸": note,
                            "ì¥ë¹„ID": equip_id,
                            "ëª¨ë¸": model,
                            "ì •ë¹„ì¢…ë¥˜": maint_type,
                            "ì •ë¹„ì": maint_person
                        })

            if not recommended:
                st.warning("ê²€ìƒ‰ëœ ì‚¬ë¡€ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                # Top3 ì„ ì •
                def is_final_action(note: str):
                    return ("ì¶”ê°€ ì¡°ì¹˜" in note) or ("ì •ìƒ í™•ì¸" in note)

                final_candidates = [r for r in recommended if is_final_action(r["ì •ë¹„ë…¸íŠ¸"])]
                candidates_sorted = sorted(final_candidates, key=lambda x: x["ì„±ê³µë¥ "], reverse=True)

                top3 = []
                used_actions = set()
                used_notes = set()
                for r in candidates_sorted:
                    note_key = r["ì •ë¹„ë…¸íŠ¸"]
                    if r["ì¡°ì¹˜"] not in used_actions and note_key not in used_notes:
                        top3.append(r)
                        used_actions.add(r["ì¡°ì¹˜"])
                        used_notes.add(note_key)
                    if len(top3) == 3:
                        break

                if len(top3) < 3:
                    for r in sorted(recommended, key=lambda x: x["ì„±ê³µë¥ "], reverse=True):
                        if r["ì¡°ì¹˜"] not in used_actions and r["ì •ë¹„ë…¸íŠ¸"] not in used_notes:
                            top3.append(r)
                            used_actions.add(r["ì¡°ì¹˜"])
                            used_notes.add(r["ì •ë¹„ë…¸íŠ¸"])
                        if len(top3) == 3:
                            break

                # ----------------------
                # 3) LLM ì„¤ëª… ìƒì„±
                # ----------------------
                top3_desc = "\n".join([f"{i+1}. {r['ì¡°ì¹˜']} â€“ ì„±ê³µë¥  {r['ì„±ê³µë¥ ']}%"
                                       for i, r in enumerate(top3)])
                prompt = f"""
ë‹¤ìŒì€ ë°˜ë„ì²´ ì¥ë¹„ ì •ë¹„ ì´ìŠˆì— ëŒ€í•œ Top3 ì„±ê³µë¥  ë†’ì€ ì¡°ì¹˜ ëª©ë¡ì…ë‹ˆë‹¤.
ê° ì¡°ì¹˜ì˜ ì˜ë¯¸ì™€ íŠ¹ì§•ì„ ìì—°ìŠ¤ëŸ½ê²Œ í•œ ë‹¨ë½ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜.
ì„±ê³µë¥  ìˆ˜ì¹˜ëŠ” ë§í•˜ì§€ ë§ˆ.

{top3_desc}
                """
                explanation = llm.predict(prompt)
                top3_html = top3_desc.replace("\n", "<br>")

                # ì±—ë´‡ ë§í’ì„ 
                st.markdown(f"""
                <div style="display:flex; justify-content:flex-start; margin-bottom:10px;">
                    <div style="font-size:30px; margin-right:8px;">ğŸ¤–</div>
                    <div style="background-color:#F1F0F0; color:black; padding:10px 15px;
                                border-radius:15px; max-width:80%;">
                        âœ… Top3 ì„±ê³µë¥  ë†’ì€ ì¡°ì¹˜<br><br>
                        {top3_html}<br><br>
                        ğŸ’¡ {explanation}
                    </div>
                </div>
                """, unsafe_allow_html=True)

                # ----------------------
                # 4) ìƒì„¸ë³´ê¸°
                # ----------------------
                for idx, r in enumerate(top3, 1):
                    with st.expander(f"ğŸ”¹ Top{idx} ìƒì„¸ë³´ê¸° ({r['ì¡°ì¹˜']}, ì„±ê³µë¥  {r['ì„±ê³µë¥ ']}%)"):
                        note_html = r['ì •ë¹„ë…¸íŠ¸'].replace("\n", "<br>")
                        st.markdown(f"""
                        <div style="border:2px solid #B0C4DE; border-radius:8px;
                                    padding:15px; margin-bottom:10px;
                                    background-color:#F3F7FF;">
                            <b>ì¡°ì¹˜ëª…:</b> {r['ì¡°ì¹˜']}<br>
                            <b>ì¥ë¹„:</b> {r['ì¥ë¹„ID']} / {r['ëª¨ë¸']}<br>
                            <b>ì •ë¹„ì¢…ë¥˜:</b> {r['ì •ë¹„ì¢…ë¥˜']}<br>
                            <b>ì •ë¹„ì:</b> {r['ì •ë¹„ì']}<br><br>
                            <b>ì •ë¹„ë…¸íŠ¸:</b><br>
                            <div style="border:1px solid #ccc; padding:10px; margin-top:5px; background:#fff;">
                                {note_html}
                            </div>
                        </div>
                        """, unsafe_allow_html=True)

# ----------------------
# 8. ì •ë¹„ í†µê³„ í˜ì´ì§€
# ----------------------
elif menu == "ğŸ“ˆ ì •ë¹„ í†µê³„ ìë£Œ":
    st.subheader("ğŸ“ˆ ì •ë¹„ í†µê³„ ìë£Œ")

    tab1, tab2, tab3 = st.tabs(["ğŸ† Top5 ìš”ì•½", "ğŸ“Š ì „ì²´ ìš”ì•½", "ğŸ”¹ ì¥ë¹„ë³„ ìƒì„¸"])

    # ----------------------
    # Tab1: Top5
    # ----------------------
    with tab1:
        with st.spinner("ğŸ“Š Top5 ìš”ì•½ ë°ì´í„°ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."):
            st.subheader("ğŸ”§ ê°€ì¥ ë§ì´ ê³ ì¥ë‚œ ì¥ë¹„ TOP5")
            top5_equip = df['ëª¨ë¸'].value_counts().head(5)

            fig1 = px.pie(
                names=top5_equip.index.tolist(),
                values=top5_equip.values,
                hole=0.4
            )
            fig1.update_traces(textinfo='percent+label')
            st.plotly_chart(fig1, use_container_width=True)

            prompt_cause = f"ë¬¸ì œ ì›ì¸: {', '.join(top5_equip.index)}\nê° ì¥ë¹„ì˜ ê³ ì¥ íŒ¨í„´ê³¼ ë°œìƒ ê²½í–¥ì„ ë°”íƒ•ìœ¼ë¡œ, ì˜ˆë°© ì •ë¹„ì™€ ê³µì • ìš´ì˜ ì¸¡ë©´ì—ì„œ ì–»ì„ ìˆ˜ ìˆëŠ” í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.ìˆ«ìëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”. 1~3ìœ„ ì •ë„ëŠ” ì¥ë¹„ë„ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•´ ì£¼ì„¸ìš”."
            insight_cause = llm.predict(prompt_cause)
            st.markdown(f"ğŸ’¡ **ë¬¸ì œ ì›ì¸ ì¸ì‚¬ì´íŠ¸:** {insight_cause}")

            # ë¬¸ì œì›ì¸ 10ê°œ ì •ì˜
            problem_keywords = [
                "wafer not", "plasma ignition failure", "pumpdown ì‹œê°„ ì§€ì—°",
                "mass flow controller ì´ìƒ", "etch residue over spec",
                "temperature abnormal", "slot valve ë™ì‘ ë¶ˆëŸ‰",
                "chamber leak", "sensor error", "RF auto match ë¶ˆëŸ‰"
            ]

            # TF-IDF ê¸°ë°˜ ë¬¸ì œì›ì¸ ë¶„ë¥˜ (ê¸°íƒ€ ì œê±°)
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.metrics.pairwise import cosine_similarity

            notes = df['ì •ë¹„ë…¸íŠ¸'].astype(str).str.lower().tolist()
            corpus = notes + [kw.lower() for kw in problem_keywords]

            vectorizer = TfidfVectorizer()
            tfidf_matrix = vectorizer.fit_transform(corpus)

            note_vecs = tfidf_matrix[:-len(problem_keywords)]
            keyword_vecs = tfidf_matrix[-len(problem_keywords):]

            similarity_matrix = cosine_similarity(note_vecs, keyword_vecs)
            best_match_indices = similarity_matrix.argmax(axis=1)
            df['ë¬¸ì œì›ì¸'] = [problem_keywords[i] for i in best_match_indices]

            st.subheader("âš ï¸ ë¬¸ì œ ì›ì¸ TOP5")
            top5_cause = df['ë¬¸ì œì›ì¸'].value_counts().head(5)

            fig2 = px.bar(
                x=top5_cause.values,
                y=top5_cause.index,
                orientation='h',
                text=top5_cause.values,
                color=top5_cause.values,
                color_continuous_scale='OrRd'
            )
            fig2.update_traces(textposition='outside')
            st.plotly_chart(fig2, use_container_width=True)

            prompt_cause = f"ë¬¸ì œ ì›ì¸: {', '.join(top5_cause.index)}\nê° ë¬¸ì œ ì›ì¸ì˜ ì˜í–¥ê³¼ ì¸ì‚¬ì´íŠ¸ë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ ì¤„ê¸€ ìš”ì•½í•´ì¤˜."
            insight_cause = llm.predict(prompt_cause)
            st.markdown(f"ğŸ’¡ **ë¬¸ì œ ì›ì¸ ì¸ì‚¬ì´íŠ¸:** {insight_cause}")

    # ----------------------
    # Tab2: ì „ì²´ ìš”ì•½
    # ----------------------
    with tab2:
        with st.spinner("ğŸ“Š ì „ì²´ ìš”ì•½ ë°ì´í„°ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."):
            st.markdown("### ğŸ“Š ì „ì²´ ìš”ì•½")
            st.subheader("ğŸ§­ ì „ì²´ ì¥ë¹„ ê³ ì¥ ë¶„í¬")

            total_equip = df['ëª¨ë¸'].value_counts()

            fig_all = px.pie(
                names=total_equip.index.tolist(),
                values=total_equip.values,
                title="ì „ì²´ ì¥ë¹„ë³„ ê³ ì¥ ë¹„ìœ¨",
                hole=0.4
            )
            fig_all.update_traces(textinfo='percent+label')
            st.plotly_chart(fig_all, use_container_width=True)

            prompt_cause = f"ë¬¸ì œ ì›ì¸: {', '.join(top5_equip.index)}\nëª¨ë“  ì¥ë¹„ì˜ ë¬¸ì œ ë°œìƒì„ ì „ì²´ì ì¸ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚¸ ê·¸ë˜í”„ì…ë‹ˆë‹¤. í•´ë‹¹ ë¹„ìœ¨ì„ ë¶„ì„í•´ë´¤ì„ ë•Œ, ì–»ì„ ìˆ˜ ìˆëŠ” ì¥ë¹„ì˜ ë¬¸ì œ ë°œìƒ ë¹„ìœ¨ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ 2~3ì¤„ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”. ìˆ«ìëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”. 1~3ìœ„ ì •ë„ëŠ” ì¥ë¹„ë„ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•´ ì£¼ì„¸ìš”."
            insight_cause = llm.predict(prompt_cause)
            st.markdown(f"ğŸ’¡ **ë¬¸ì œ ì›ì¸ ì¸ì‚¬ì´íŠ¸:** {insight_cause}")

            st.subheader("ğŸ§  ì „ì²´ ë¬¸ì œ ì›ì¸ ë¶„í¬")

            total_cause = df['ë¬¸ì œì›ì¸'].value_counts()

            fig_cause = px.pie(
                names=total_cause.index.tolist(),
                values=total_cause.values,
                title="ì „ì²´ ë¬¸ì œ ì›ì¸ë³„ ê³ ì¥ ë¹„ìœ¨",
                hole=0.4
            )
            fig_cause.update_traces(textinfo='percent+label')
            st.plotly_chart(fig_cause, use_container_width=True)

            prompt_cause = f"ë¬¸ì œ ì›ì¸: {', '.join(top5_equip.index)}\nëª¨ë“  ë¬¸ì œì˜ ì›ì¸ì„ ì „ì²´ì ì¸ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚¸ ê·¸ë˜í”„ì…ë‹ˆë‹¤. í•´ë‹¹ ë¹„ìœ¨ì„ ë¶„ì„í•´ë´¤ì„ ë•Œ, ì–»ì„ ìˆ˜ ìˆëŠ” ë¬¸ì œ ë°œìƒì›ì¸ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ 2~3ì¤„ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”. ìˆ«ìëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”. 1~3ìœ„ ì •ë„ëŠ” ë¬¸ì œ ì›ì¸ë„ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•´ ì£¼ì„¸ìš”."
            insight_cause = llm.predict(prompt_cause)
            st.markdown(f"ğŸ’¡ **ë¬¸ì œ ì›ì¸ ì¸ì‚¬ì´íŠ¸:** {insight_cause}")

    # ----------------------
    # Tab3: ì¥ë¹„ë³„ ìƒì„¸
    # ----------------------
    with tab3:
        with st.spinner("ğŸ“Š ì¥ë¹„ë³„ ìƒì„¸ ë°ì´í„°ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."):
            st.markdown("### ğŸ”¹ ì¥ë¹„ë³„ ìƒì„¸")

            problem_keywords = [
                "wafer not", "plasma ignition failure", "pumpdown ì‹œê°„ ì§€ì—°",
                "mass flow controller ì´ìƒ", "etch residue over spec",
                "temperature drift", "slot valve ë™ì‘ ë¶ˆëŸ‰",
                "chamber pressure fluctuation", "he flow deviation", "RF auto match ë¶ˆëŸ‰"
            ]
            alias_map = {
                "wafer not ê°ì§€ë¨": "wafer not",
                "wafer not ë°œìƒ": "wafer not",
                "rf auto match fail": "RF auto match ë¶ˆëŸ‰",
                "slot valve ë¶ˆëŸ‰": "slot valve ë™ì‘ ë¶ˆëŸ‰",
                "he flow dev": "he flow deviation",
            }

            def normalize_note(note: str) -> str:
                note = str(note).lower()
                note = re.sub(r'\s+', ' ', note)
                return note

            def extract_cause(note: str):
                note_low = normalize_note(note)
                for alias, norm in alias_map.items():
                    if alias in note_low:
                        return norm
                for keyword in problem_keywords:
                    if keyword.lower() in note_low:
                        return keyword
                return "ê¸°íƒ€"

            df['ë¬¸ì œì›ì¸'] = df['ì •ë¹„ë…¸íŠ¸'].apply(extract_cause)

            equip_list = df['ëª¨ë¸'].dropna().unique().tolist()
            selected_equip = st.selectbox("ì¥ë¹„ë¥¼ ì„ íƒí•˜ì„¸ìš”", ["ì „ì²´"] + equip_list)

            if selected_equip != "ì „ì²´":
                df_filtered = df[df['ëª¨ë¸'] == selected_equip]

                if df_filtered.empty:
                    st.warning(f"ì„ íƒí•œ ì¥ë¹„({selected_equip})ì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    top5_cause_equip = (
                        df_filtered.groupby('ë¬¸ì œì›ì¸')
                        .size()
                        .reset_index(name='ë¹ˆë„')
                        .sort_values('ë¹ˆë„', ascending=False)
                        .head(5)
                    )

                    st.subheader(f"âš¡ {selected_equip} ë¬¸ì œ ì›ì¸ TOP5")
                    fig3 = px.bar(
                        x=top5_cause_equip['ë¹ˆë„'],
                        y=top5_cause_equip['ë¬¸ì œì›ì¸'],
                        orientation='h',
                        text=top5_cause_equip['ë¹ˆë„'],
                        color=top5_cause_equip['ë¹ˆë„'],
                        color_continuous_scale='Purples'
                    )
                    fig3.update_traces(textposition='outside')
                    st.plotly_chart(fig3, use_container_width=True)

                    selected_cause = st.selectbox(
                        "ë¬¸ì œ ì›ì¸ì„ ì„ íƒí•˜ì„¸ìš”",
                        top5_cause_equip['ë¬¸ì œì›ì¸'].tolist()
                    )

                    if selected_cause:
                        df_cause = df_success[df_success['ëŒ€í‘œì›ì¸'] == selected_cause]
                        top3_actions = (
                            df_cause.sort_values('ì„±ê³µë¥ (%)', ascending=False)
                            .head(3)[['ì¡°ì¹˜']]
                        )

                        if top3_actions.empty:
                            st.info(f"'{selected_cause}'ì— ëŒ€í•œ ì¶”ì²œ ì¡°ì¹˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        else:
                            st.markdown(f"### ğŸ¤– '{selected_cause}' ì¶”ì²œ ì¡°ì¹˜ TOP3")
                            for idx, row in top3_actions.iterrows():
                                st.markdown(f"- {row['ì¡°ì¹˜']}")
            else:
                st.info("ì¥ë¹„ë¥¼ ì„ íƒí•˜ë©´ í•´ë‹¹ ì¥ë¹„ì˜ ë¬¸ì œ ì›ì¸ê³¼ ì¶”ì²œ ì¡°ì¹˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
